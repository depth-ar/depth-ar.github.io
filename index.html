<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Scalable Autoregressive Monocular Depth Estimation</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Scalable Autoregressive Monocular Depth Estimation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Jinhong Wang</a><sup>1*</sup>,</span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Jian Liu</a><sup>2</sup>,</span>
                  <span class="author-block">
                      <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Dongqi Tang</a><sup>2</sup>,</span>
                      <span class="author-block">
                        <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Weiqiang Wang</a><sup>2</sup>,</span>
                        <span class="author-block">
                          <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Wentong Li</a><sup>1</sup>,</span>
                            <span class="author-block">
                            <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Danny Chen</a><sup>3</sup>,</span><br>
                              <span class="author-block">
                              <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Jintai Chen</a><sup>4✝</sup>,</span>
                                <span class="author-block">
                                  <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Jian Wu</a><sup>1✝</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><pre><sup>1</sup>ZJU   <sup>2</sup>Ant Group   <sup>3</sup>University of Notre Dame   <sup>4</sup>HKUST (Guangzhou)<br>CVPR 2025</pre></span>
                    <span class="eql-cntrb"><small><br><sup>✝</sup>Corresponding Authors</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2411.11361" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper (comming soon)</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary (comming soon)</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/wjh892521292/DAR" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (comming soon)</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2411.11361" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
 <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/overview.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        We introduce the first autoregressive model for monocular depth estimation (MDE), called DAR — a simple, effective, and scalable framework. Our key insight lies in transforming two ordered properties in MDE, depth map resolution and granularity, into autoregressive objectives. By recast MDE as a coarse-to-fine autoregressive objective process, DAR can be easily scaled up to larger size to obtain better performance and generalization.
      </h2>
    </div>
  </div>
</section> 
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            This paper shows that the autoregressive model is an effective and scalable monocular depth estimator. Our idea is simple: We tackle the monocular depth estimation (MDE) task with an autoregressive prediction paradigm, based on two core designs. First, our depth autoregressive model (DAR) treats the depth map of different resolutions as a set of tokens, and conducts the low-to-high resolution autoregressive objective with a patch-wise casual mask. Second, our DAR recursively discretizes the entire depth range into more compact intervals, and attains the coarse-to-fine granularity autoregressive objective in an ordinal-regression manner. By coupling these two autoregressive objectives, our DAR establishes new state-of-the-art (SOTA) on KITTI and NYU Depth v2 by clear margins. Further, our scalable approach allows us to scale the model up to 2.0B and achieve the best RMSE of 1.799 on the KITTI dataset (5% improvement) compared to 1.896 by the current SOTA (Depth Anything). DAR further showcases zero-shot generalization ability on unseen datasets. These results suggest that DAR yields superior performance with an autoregressive prediction paradigm, providing a promising approach to equip modern autoregressive large models (e.g., GPT-4o) with depth estimation capabilities.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->



<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Motivation: Autoregressive Objective</h2>

          <div class="columns">
            <div class="column is-two-thirds">
              <p>We exploit two “order” properties of the MDE task that can be transformed into two autoregressive objectives. (a) Resolution autoregressive objective: The generation of depth maps can follow a resolution order from low to high. For each step of the resolution autoregressive process, the Transformer predicts the next higher-resolution token map conditioned on all the previous ones. (b) Granularity autoregressive objective: The range of depth values is ordered, from 0 to specific max values. For each step of the granularity autoregressive process, we increase exponentially the number of bins (e.g., doubling the bin number), and utilize the previous predictions to predict a more refined depth with a smaller and more refined granularity. 
              </p>
              <p>Motivated by this, we propose DAR, which cast the MDE into autoregressive framework that aims to perform these two autoregressive processes simultaneously.
              </p>
            </div>
            <div class="column is-one-third">
              <img src="static/images/motivation.jpg" alt="Weight distance correlation" class="blend-img-background center-image" style="max-width: 100%; height: auto;" />
              <em><strong>Motivation:</strong></em> Two 'order' properties that naturally become autoregressive objectives.
            </div>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>



<section class="section hero is-small is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Largest Model Trees on Hugging Face</h2>
          <div class="level-set has-text-justified">
<p>
                    <img src="/probex/static/images/model_sizes.png" alt="Sizes of model trees" class="blend-img-background center-image" style="max-width: 100%; height: auto;" />
          </p>
              <p>
                We show the 10 largest Model Trees on Hugging Face. Our insight is that learning an expert for each tree greatly simplifies weight-space learning. This is a practical setting as a few large Model Trees dominate the landscape.
              </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Probing Experts (ProbeX)</h2>
          <p>
          <img src="/probex/static/images/ProbeX_overview.png" alt="ProbeX Overview" class="blend-img-background center-image" style="max-width: 85%; height: auto;" />
          </p>
           <p>
              Unlike conventional probing methods that operate only on inputs and outputs, our lightweight architecture scales weight-space learning to large models by probing hidden model layers. ProbeX begins by passing a set of learned probes, 
              <math xmlns="http://www.w3.org/1998/Math/MathML">
                <mi mathvariant="bold">u</mi><mo>_</mo><mn>1</mn><mo>,</mo><mi mathvariant="bold">u</mi><mo>_</mo><mn>2</mn><mo>,</mo><mo>&#x2026;</mo><mo>,</mo><mi mathvariant="bold">u</mi><mo>_</mo><mi>r</mi><mo>_</mo><mi>U</mi>
              </math>, through the input weight matrix 
              <math xmlns="http://www.w3.org/1998/Math/MathML">
                <mi>X</mi>
              </math>. A projection matrix 
              <math xmlns="http://www.w3.org/1998/Math/MathML">
                <mi>V</mi>
              </math>, shared between all probes, reduces the dimensionality of the probe responses, followed by a non-linear activation. Each probe response is then mapped to a probe encoding 
              <math xmlns="http://www.w3.org/1998/Math/MathML">
                <mi mathvariant="bold">e</mi><mo>_</mo><mi>l</mi>
              </math> via a per-probe encoder matrix 
              <math xmlns="http://www.w3.org/1998/Math/MathML">
                <mi>M</mi><mo>_</mo><mi>l</mi>
              </math>. We sum the probe encodings to obtain the final model encoding 
              <math xmlns="http://www.w3.org/1998/Math/MathML">
                <mi mathvariant="bold">e</mi>
              </math>, which the predictor head maps to the task output 
              <math xmlns="http://www.w3.org/1998/Math/MathML">
                <mi mathvariant="bold">y</mi>
              </math>.
            </p>    
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-small is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Results</h2>
            <p>
              ProbeX achieves state-of-the-art results on the task of training category prediction, accurately identifying the specific classes within a model’s training dataset. Excitingly, ProbeX can also align fine-tuned Stable Diffusion weights with language representations. This capability enables a new task: zero-shot model classification, where models are classified via a text prompt describing their training data. Using these aligned representations, ProbeX can also perform model retrieval.
            </p>
              <img src="/probex/static/images/retrieval.png" alt="Model retrieval" class="blend-img-background center-image" style="max-width: 85%; height: auto;" />
        </div>
      </div>
    </div>
  </div>
</section>

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{wang2024scalable,
        title={Scalable Autoregressive Monocular Depth Estimation},
        author={Wang, Jinhong and Liu, Jian and Tang, Dongqi and Wang, Weiqiang and Li, Wentong and Chen, Danny and Chen, Jintai and Wu, Jian},
        journal={arXiv preprint arXiv:2411.11361},
        year={2024}
      }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
