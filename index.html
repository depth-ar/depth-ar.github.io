<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Scalable Autoregressive Monocular Depth Estimation</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Scalable Autoregressive Monocular Depth Estimation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Jinhong Wang</a><sup>1*</sup>,</span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Jian Liu</a><sup>2</sup>,</span>
                  <span class="author-block">
                      <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Dongqi Tang</a><sup>2</sup>,</span>
                      <span class="author-block">
                        <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Weiqiang Wang</a><sup>2</sup>,</span>
                        <span class="author-block">
                          <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Wentong Li</a><sup>1</sup>,</span>
                            <span class="author-block">
                            <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Danny Chen</a><sup>3</sup>,</span><br>
                              <span class="author-block">
                              <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Jintai Chen</a><sup>4✝</sup>,</span>
                                <span class="author-block">
                                  <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Jian Wu</a><sup>1✝</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><pre><sup>1</sup>ZJU   <sup>2</sup>Ant Group   <sup>3</sup>University of Notre Dame   <sup>4</sup>HKUST (Guangzhou)<br>CVPR 2025</pre></span>
                    <span class="eql-cntrb"><small><br><sup>✝</sup>Corresponding Authors</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_Scalable_Autoregressive_Monocular_Depth_Estimation_CVPR_2025_paper.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="https://openaccess.thecvf.com/content/CVPR2025/supplemental/Wang_Scalable_Autoregressive_Monocular_CVPR_2025_supplemental.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/wjh892521292/DAR" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2411.11361" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
 <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/overview.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        We introduce the first autoregressive model for monocular depth estimation (MDE), called DAR — a simple, effective, and scalable framework. Our key insight lies in transforming two ordered properties in MDE, depth map resolution and granularity, into autoregressive objectives. By recast MDE as a coarse-to-fine autoregressive objective process, DAR can be easily scaled up to larger size to obtain better performance and generalization.
      </h2>
    </div>
  </div>
</section> 
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            This paper shows that the autoregressive model is an effective and scalable monocular depth estimator. Our idea is simple: We tackle the monocular depth estimation (MDE) task with an autoregressive prediction paradigm, based on two core designs. First, our depth autoregressive model (DAR) treats the depth map of different resolutions as a set of tokens, and conducts the low-to-high resolution autoregressive objective with a patch-wise casual mask. Second, our DAR recursively discretizes the entire depth range into more compact intervals, and attains the coarse-to-fine granularity autoregressive objective in an ordinal-regression manner. By coupling these two autoregressive objectives, our DAR establishes new state-of-the-art (SOTA) on KITTI and NYU Depth v2 by clear margins. Further, our scalable approach allows us to scale the model up to 2.0B and achieve the best RMSE of 1.799 on the KITTI dataset (5% improvement) compared to 1.896 by the current SOTA (Depth Anything). DAR further showcases zero-shot generalization ability on unseen datasets. These results suggest that DAR yields superior performance with an autoregressive prediction paradigm, providing a promising approach to equip modern autoregressive large models (e.g., GPT-4o) with depth estimation capabilities.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->



<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Motivation: Where is Autoregressive Objectives？</h2>

          <div class="columns">
            <div class="column is-two-thirds">
              <p>We exploit two “order” properties of the MDE task that can be transformed into two autoregressive objectives. (a) Resolution autoregressive objective: The generation of depth maps can follow a resolution order from low to high. For each step of the resolution autoregressive process, the Transformer predicts the next higher-resolution token map conditioned on all the previous ones. (b) Granularity autoregressive objective: The range of depth values is ordered, from 0 to specific max values. For each step of the granularity autoregressive process, we increase exponentially the number of bins (e.g., doubling the bin number), and utilize the previous predictions to predict a more refined depth with a smaller and more refined granularity. 
              </p>
              <p>Motivated by this, we propose DAR, which cast the MDE into autoregressive framework that aims to perform these two autoregressive processes simultaneously.
              </p>
            </div>
            <div class="column is-one-third">
              <img src="static/images/motivation.jpg" alt="Weight distance correlation" class="blend-img-background center-image" style="max-width: 110%; height: auto;" />
              <em><strong>Motivation:</strong></em> Two 'order' properties that naturally become autoregressive objectives.
            </div>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>



<section class="section hero is-small is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Overview: Our method</h2>
          <div class="level-set has-text-justified">
<p>
                    <img src="static\images\overview.png" alt="Sizes of model trees" class="blend-img-background center-image" style="max-width: 100%; height: auto;" />
          </p>
              <p>
                An overview of DAR.  We begin with encoding the input RGB images into image tokens as the context condition.  At each step, DAR Transformer with the patch-wise causal mask performs autoregressive predictions, that is, it allows the input token map (upsampled from the previous resolution token map rk−1) to interact with only the prefix tokens and global image feature tokens for the next-resolution token map modeling.  The output latent tokens are then sent to the ConvGRU module, which injects the prompts of new refined bin candidates ck (generated by MTBin from the previous prediction ̃Dk−1) for further granularity guidance and generates the next-resolution token map rk .  The new depth map ̃Dk is generated by a linear combination of the next-granularity bin candidates ck and softmax value pk of the next-resolution token map, achieving concurrently a resolution and granularity autoregressive evolution.</p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Scalable MDE Model</h2>

          <div class="columns">
            <div class="column is-two-thirds">
              <p>Our DAR shows strong scalability and achieves better performance-efficiency trade-off among cutting-edge methods.</p>
              <img src="static\images\tt.png" alt="Sizes of model trees" class="blend-img-background center-image" style="max-width: 100%; height: auto;" />
            </div>
            
            <div class="column is-one-third">
              <img src="static/images/ss.png" alt="Weight distance correlation" class="blend-img-background center-image" style="max-width: 150%; height: auto;" />
                   RMSE performances (↓) vs. model sizes on the KITTI dataset.
            </div>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>
<!-- 
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Probing Experts (ProbeX)</h2>
          <p>
          <img src="/probex/static/images/ProbeX_overview.png" alt="ProbeX Overview" class="blend-img-background center-image" style="max-width: 85%; height: auto;" />
          </p>
           <p>
              Unlike conventional probing methods that operate only on inputs and outputs, our lightweight architecture scales weight-space learning to large models by probing hidden model layers. ProbeX begins by passing a set of learned probes, 
              <math xmlns="http://www.w3.org/1998/Math/MathML">
                <mi mathvariant="bold">u</mi><mo>_</mo><mn>1</mn><mo>,</mo><mi mathvariant="bold">u</mi><mo>_</mo><mn>2</mn><mo>,</mo><mo>&#x2026;</mo><mo>,</mo><mi mathvariant="bold">u</mi><mo>_</mo><mi>r</mi><mo>_</mo><mi>U</mi>
              </math>, through the input weight matrix 
              <math xmlns="http://www.w3.org/1998/Math/MathML">
                <mi>X</mi>
              </math>. A projection matrix 
              <math xmlns="http://www.w3.org/1998/Math/MathML">
                <mi>V</mi>
              </math>, shared between all probes, reduces the dimensionality of the probe responses, followed by a non-linear activation. Each probe response is then mapped to a probe encoding 
              <math xmlns="http://www.w3.org/1998/Math/MathML">
                <mi mathvariant="bold">e</mi><mo>_</mo><mi>l</mi>
              </math> via a per-probe encoder matrix 
              <math xmlns="http://www.w3.org/1998/Math/MathML">
                <mi>M</mi><mo>_</mo><mi>l</mi>
              </math>. We sum the probe encodings to obtain the final model encoding 
              <math xmlns="http://www.w3.org/1998/Math/MathML">
                <mi mathvariant="bold">e</mi>
              </math>, which the predictor head maps to the task output 
              <math xmlns="http://www.w3.org/1998/Math/MathML">
                <mi mathvariant="bold">y</mi>
              </math>.
            </p>    
        </div>
      </div>
    </div>
  </div>
</section> -->

<section class="section hero is-small is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Results</h2>
            <p>
              We show the visualization performance of DAR on the MDE task. First, one can observe that our model performs better in depth estimation at the boundaries of the objects, making it more coherent and smooth (e.g., the back of the chair and the longdistant objects). This is helped by our autoregressive progressive paradigm, which maintains a coherent and smooth depth estimation when using previous predictions for the next-step, more refined prediction. Second, our DAR is much more accurate when estimating the depths of small and thin objects or long-distant visually relatively small objects, like the poles under the chair. These observations further demonstrate the superiority of our DAR. One can observe that DAR preserves fine-grained boundary details and generates more continuous depth values, further demonstrating the effectiveness of our new AR-based framework.</p>
              <img src="static\images\v1.png" alt="Model retrieval" class="blend-img-background center-image" style="max-width: 85%; height: auto;" />
              <img src="static\images\v2.png" alt="Model retrieval2" class="blend-img-background center-image" style="max-width: 85%; height: auto;" />
            </div>
      </div>
    </div>
  </div>
</section>

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{wang2024scalable,
        title={Scalable Autoregressive Monocular Depth Estimation},
        author={Wang, Jinhong and Liu, Jian and Tang, Dongqi and Wang, Weiqiang and Li, Wentong and Chen, Danny and Chen, Jintai and Wu, Jian},
        journal={arXiv preprint arXiv:2411.11361},
        year={2024}
      }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
